import torch
import torch.nn.functional as F
import os
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    T5Tokenizer,
    T5ForConditionalGeneration,
)

sm = torch.nn.LogSoftmax(dim=1)
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)
logging.getLogger().setLevel(logging.INFO)


import numpy as np
import os


# from dotenv import load_dotenv
from retry import retry

from Data_generation.templates import get_possible_answers
from Predict.Predictor import Predictor


class T5Predictor(Predictor):
    def __init__(
        self,
        bias_name,
        engine,
        max_tokens,
        predict_according_to_log_probs,
        should_normalize,
        save_every_n_examples,
    ):
        super().__init__(
            bias_name,
            engine,
            max_tokens,
            predict_according_to_log_probs,
            should_normalize,
            save_every_n_examples,
        )

    def set_parameters(self):
        super().set_parameters()
        self.load_t5_model_and_tokenizer()

    def get_scores_for_labels(self, input, labels):
        batch_size, num_labels = len(input), len(labels)
        # Get encodings
        input_enc = self.tokenizer.batch_encode_plus(
            input,
            return_tensors="pt",
            add_special_tokens=True,
            truncation=True,
            padding="longest",
        )
        target_enc = self.tokenizer.batch_encode_plus(
            labels, return_tensors="pt", padding="longest"
        ).input_ids

        for k, v in input_enc.items():
            input_enc[k] = v.to(self.model.device)
        target_enc = target_enc.to(self.model.device)

        # Get encoder's last hidden state
        encoder_hidden_states = self.model.encoder(**input_enc)[0]

        # Repeat the inputs `num_label` times
        encoder_hidden_states = (
            encoder_hidden_states.unsqueeze(dim=1)
            .repeat(1, num_labels, 1, 1)
            .flatten(0, 1)
        )
        attention_mask = (
            input_enc.attention_mask.unsqueeze(dim=1)
            .repeat(1, num_labels, 1)
            .flatten(0, 1)
        )

        # Create the decoding mask (that is commonly generated by the T5 model at predict time) -- makes it more efficient
        decoder_input_ids = torch.cat(
            [
                torch.zeros(
                    (num_labels * batch_size, 1),
                    dtype=torch.int,
                    device=self.model.device,
                ),
                target_enc[:, :-1].repeat(batch_size, 1),
            ],
            dim=1,
        )
        decoder_attention_mask = (decoder_input_ids == decoder_input_ids).float()
        lm_target = (
            target_enc - 100 * (target_enc == self.tokenizer.pad_token_id).long()
        )

        model_output = self.model(
            attention_mask=attention_mask,
            encoder_outputs=[encoder_hidden_states],
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
        )

        # Compute the log probabilities associated with each of the labels
        labels_log_probs = F.cross_entropy(
            model_output.logits.flatten(0, 1),
            lm_target.repeat(batch_size, 1).flatten(0, 1),
            reduction="none",
        )

        # Sum log probs for each of the (input, label) pair
        labels_scores = labels_log_probs.view(batch_size, num_labels, -1)
        # We don't want to sum over the sequence dimension, just get the last token score (not including eos token)
        # labels_scores = labels_scores.sum(dim=-1)
        labels_scores = labels_scores[:, :, -2]

        # Note: Label log probabilities are positive (due to the internals of pytorch's
        # cross entropy). To obtain the "logits", we need to multiply by -1.
        return labels_scores * -1

    def get_t5_maximum_logprobs_answer(
        self,
        prompt,
    ):
        labels = [t[0] for t in self.possible_answers]

        # Get the scores for each possible answer
        labels_scores = self.get_scores_for_labels([prompt], labels)

        if self.should_normalize:
            if not self.base_probs:
                self.base_probs["base_probs"] = self.get_scores_for_labels(
                    [prompt.split("\n")[-1]], labels
                )

            labels_scores = labels_scores - self.base_probs["base_probs"]
        # Get the maximum score for each input
        max_scores = labels_scores.max(dim=-1)[0]
        # Get the index of the maximum score for each input
        max_scores_idx = labels_scores.argmax(dim=-1)
        # Get the answer corresponding to the maximum score
        max_scores_answers = self.possible_answers[max_scores_idx][0]
        # Get the log probability of the maximum score
        max_scores_logprobs = max_scores - labels_scores.logsumexp(dim=-1)
        # Get the normalized log probability of the maximum score
        # max_scores_logprobs_norm = max_scores_logprobs - max_scores_logprobs.logsumexp(dim=-1)

        return max_scores_answers, max_scores_logprobs.item()

    def get_t5_prediction(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = inputs.to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=self.max_tokens,
            return_dict_in_generate=True,
            output_scores=True,
        )

        pred_text = self.tokenizer.batch_decode(
            outputs.sequences, skip_special_tokens=True
        )
        log_probs, tokens_id = sm(torch.cat(outputs["scores"], dim=0)).max(dim=1)
        logits = {id.item(): p.item() for id, p in zip(tokens_id, log_probs)}

        return pred_text[0].strip().strip("."), logits

    def load_t5_model_and_tokenizer(
        self,
    ):
        cwd = os.getcwd()
        cache_dir = cwd + "/huggingface/.cache"
        os.makedirs(cache_dir, exist_ok=True)

        model_name = self.parameters["engine"]
        logging.info(f"Loading model and tokenizer for {model_name}")
        if torch.cuda.is_available():
            torch_dtype = torch.float32  # float32##
            device = torch.device("cuda")
        else:
            torch_dtype = torch.float32
            device = torch.device("cpu")
        logging.info(f"Using device {device} {torch_dtype}")

        if "flan" in model_name:
            model = AutoModelForSeq2SeqLM.from_pretrained(
                f"google/{model_name}",
                cache_dir=cache_dir,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )  # , device_map="auto"#, load_in_8bit=True
            # )
            tokenizer = AutoTokenizer.from_pretrained(f"google/{model_name}")
        else:  # vanilla t51.1
            model = T5ForConditionalGeneration.from_pretrained(
                f"google/{model_name}",
                cache_dir=cache_dir,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )  # ,device_map='balanced')#, device_map="auto"#, load_in_8bit=True
            # )
            tokenizer = T5Tokenizer.from_pretrained(f"google/{model_name}")
        if torch.cuda.is_available():
            print(f"{model.device=}")
            model.to("cuda")
            model.parallelize()
            print("parallelizing...")
            print(f"{model.device=}")
        if "cpu" in str(model.device):
            print(f"NO GPU! {model.device=}")
        else:
            print(f"Using GPU. {model.device=}")

        self.model = model
        self.tokenizer = tokenizer

    def predict(
        self,
        example,
        prompt,
    ):
        prediction = dict()
        prediction["input"] = prompt

        if self.possible_answers:
            prediction_text, prediction_log_probs = self.get_t5_maximum_logprobs_answer(
                prompt,
            )
        else:  # return model completion
            prediction_text, prediction_log_probs = self.get_t5_prediction(
                prompt,
            )

        prediction["prediction"] = prediction_text
        metadata = example.copy()
        metadata["log_probs"] = prediction_log_probs

        prediction["metadata"] = metadata
        return prediction, metadata
